# 时间序列

## 时间序列的基本

**时间序列分析的核心就是挖掘该时间序列中的自相关性。**
通过时间序列分解，可以帮助大家从时间序列的波动中挖掘信息。

### 数据的平稳性

**经典回归分析的一个重要假设是：数据是平稳的。**
非平稳数据往往导致“虚假回归”，表现为两个没有任何因果关系的变量，却有很高的相关性。
比如在时间序列中，本来没有自相关性的两个时间点，产生了相关性。因此平稳性是时间序列分析的基础。

平稳性定义（弱平稳）：

1、均值 u 是与时间 t 无关的常数

2、对于任意时刻 t 和任意时间间隔 k，时间序列 $ts(t)$ 与 $ts(t+k)$ 的自协方差 $R(t, t+k)$ 只与 k 有关，与 t 无关。

从统计学的角度讲，**平稳性的要求就是对于一个时间序列（分布未知）**，这个时间序列的取值一定满足一个确定的分布。

**平稳性对于我们分析时间序列至关重要。如果一个时间序列不是平稳的，通常需要通过差分的方式将其转化为平稳时间序列。**

对于一个时间序列，如何确定它是否满足平稳性要求？通常采用 ADF 检验。

ADF 大致的思想就是基于随机游走的，对 $X+t$回归，如果发现p=1，说明序列满足随机游走，就是非平稳的。

根据时间序列的特点，形成了自相关函数、偏自相关函数。

自相关（autocorrelation）**，也叫序列相关，是一个信号于其自身在不同时间点的相关度**。非正式地来说，它就是两次观察之间的相似度对它们之间的时间差的函数。它是找出重复模式（如被噪声掩盖的周期信号），或识别隐含在信号谐波频率中消失的基频的数学工具。它常用于信号处理中，用来分析函数或一系列值，如时域信号。

偏自相关系数（PACF）

根据ACF求出滞后k自相关系数 $ACF(k)$ 时，实际上得到并不是Z(t)与Z(t-k)之间单纯的相关关系。

**因为Z(t)同时还会受到中间k-1个随机变量Z(t-1)、Z(t-2)、……、Z(t-k+1)的影响**，而这k-1个随机变量又都和z(t-k)具有相关关系，**所以自相关系数里面实际掺杂了其他变量对Z(t)与Z(t-k)的影响**。

为了能单纯测度Z(t-k)对Z(t)的影响，引进偏自相关系数（PACF）的概念。

**计算某一个要素对另一个要素的影响或相关程度时，把其他要素的影响视为常数，即暂不考虑其他要素的影响，而单独研究那两个要素之间的相互关系的密切程度时，称为偏相关。**

对时间序列建模，最重要的就是挖掘出该序列中的不同间隔$k$的自相关性。相关图可以帮助我们判断模型是否合适。这是因为时间序列的特征中往往包括相关性和随机噪声。如果模型很好的捕捉了自相关性，那么原始时间序列与模型拟合的时间序列之间的残差应该近似的等于随机噪声。残差序列自然也是一个时间序列，因此可以对它画出相关图。一个标准随机噪声的自相关满足  以及 [公式] , [公式] 即对于任意不为 0 的间隔，随机噪声的自相关均为 0。

## ARIMA模型

### AR（Auto Regression）模型

$$X_{t}=\alpha_{1} X_{t-1}+\alpha_{2} X_{t-2}+\ldots+\alpha_{p} X_{t-p}+u_{t}$$

如果$u_{t}=\epsilon_t$是一个白噪声，则是一个纯AR(p)过程。

自回归模型有很多的限制：
（1）自回归模型是用自身的数据进行预测
（2）时间序列数据必须具有平稳性
（3）自回归只适用于预测与自身前期相关的现象（时间序列的自相关性）

### MA (Moving Average) 模型

在AR中，如果$u_t$不是白噪声，通常认为是一个$q$阶的移动平均。$u_{t}=\varepsilon_{t}+\beta_{1} \varepsilon_{t-1}+\ldots+\beta_{q} \varepsilon_{t-q}$，$\varepsilon_{t}$为白噪声，当$X_t =\varepsilon_{t}$，跟历史值没关系，只依赖历史噪声

需要指出一点，AR模型中历史白噪声的影响是间接影响当前预测值的（通过影响历史时序值）。

### ARMA模型

将AR(p)与MA(q)结合，得到一个一般的自回归移动平均模型ARMA(p，q)

$$X_{t}=\alpha_{1} X_{t-1}+\alpha_{2} X_{t-2}+\ldots+\alpha_{p} X_{t-p}+\varepsilon_{t}+\beta_{1} \varepsilon_{t-1}+\ldots+\beta_{q} \varepsilon_{t-q}$$

该式表明：
（1）一个随机时间序列可以通过**一个自回归移动平均模型来表示，即该序列可以由其自身的过去或滞后值以及随机扰动项来解释**。
（2）如果该序列是平稳的，即它的行为并不会随着时间的推移而变化，那么我们就可以通过该序列过去的行为来预测未来。

**ARIMA:将自回归模型（AR）、移动平均模型（MA）和差分法结合，我们就得到了差分自回归移动平均模型 ARIMA（p、d、q），其中 d 是需要对数据进行差分的阶数。**

1.对序列绘图，进行 ADF 检验，观察序列是否平稳；对于非平稳时间序列要先进行 d 阶差分，转化为平稳时间序列；
2. 经过第一步处理，已经得到平稳时间序列。要对平稳时间序列分别求得其自相关系数（ACF）和偏自相关系数（PACF），通过对自相关图和偏自相关图的分析，得到最佳的阶数p、q；
3. 由以上得到的d、q、p ，得到 ARIMA 模型。然后开始对得到的模型进行模型检验。

首先画出时间序列的ACF、PACF

然后判断拖尾还是结尾

| 模型 | AR(p) | MA(q)| ARMA(p,q)|
|-----|-------|------|-------|
|ACF  | 拖尾|q阶结尾| 拖尾|
|PACF | 截尾| 拖尾| 拖尾|



通过拖尾和截尾对模型定阶，具有很强的主观性。回顾一下我们对于模型参数估计得方法，是通过对损失和正则项的加权评估。我们在参数选择的时候，需要平衡预测误差与模型复杂度。我们可以根据信息准则函数法，来确定模型的阶数。这里介绍 AIC、BIC准则。

AIC 准则全称是最小化信息量准则（Akaike Information Criterion）：

$AIC=-2ln(L)+2K$ ，其中L表示模型的极大似然函数，K表示模型参数个数。

AIC 准则存在一定的不足。当样本容量很大时，在 AIC 准则中拟合误差提供的信息就要受到样本容量的放大，而参数个数的惩罚因子却和样本容量没关系（一直是2），因此当样本容量很大时，使用 AIC 准则的模型不收敛于真实模型，它通常比真实模型所含的未知参数个数要多。BIC（Bayesian InformationCriterion）贝叶斯信息准则弥补了 AIC 的不足：$BIC=-2ln(L)+Kln(n)$ ，其中 n 表示样本容量。

通过网格搜索:`sm.tsa.arma_order_select_ic`

显然，这两个评价指标越小越好。我们通过网格搜索，确定 AIC、BIC 最优的模型（p、q）

对于训练集，拟合序列 = 周期序列 + 趋势序列（ARIMA拟合） + 残差序列（ARIMA拟合）

对于测试集，预测序列 = 周期序列 + 趋势序列（ARIMA预测） + 残差序列（ARIMA预测）

## Lowess回归

使用KNN做平均回归：$f(\hat{x})=\operatorname{Ave}\left(y_{i} | x_{i} \in N_{k}(x)\right)$，其中$N_k(x)$为距离点x最近k个点组成的领域集合(neighborhood set)。

1. 没考虑到距离不同点的权重
2. 拟合曲线不连续

因此引入kernel加权平滑：

$$f\left(\hat{x}_{0}\right)=\frac{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right) y_{i}}{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)}$$

比如，Epanechnikov 二次kernel：

$$\begin{array}{c}
K_{\lambda}\left(x_{0}, x_{i}\right)=D\left(\frac{\left|x_{0}-x_{i}\right|}{\lambda}\right) \\
D(t)=\left\{\begin{array}{cc}
\frac{3}{4}\left(1-t^{2}\right) & \text { for }|t|<1 \\
0 & \text { otherwise }
\end{array}\right.
\end{array}
$$

其中，λ为kernel的参数，称之为window width。对于kNN，只考虑最近的k个点影响；基于此，$\lambda = |x_0 - x_[k]|$其中，$x_[k]$为距离$x_0$第k近的点。如上图，经kernel加权平滑后，回归拟合的曲线为连续的了。但是，这种kernel回归童颜存在着边界的问题。对于x序列的开始与结束区段的点，其左右邻域是不对称的，导致了平滑后的值偏大或偏小。因此，需要对权值做再修正，假定对x0的估计值：$f\left(\hat{x}_{0}\right)=\sum_{j=0}^{d} \beta_{j} x_{0}^{j}$目标函数为$\min _{\beta} \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left[y_{i}-\sum_{j=0}^{d} \beta_{j} x_{i}^{j}\right]^{2}$

## Robust LOWESS

Robust LOWESS是Cleveland在LOWESS基础上提出来的robust回归方法，能避免outlier对回归的影响。在计算完估计值后，计算残差：

$$e_{i}=y_{i}-f\left(x_{i}\right)$$

根据残差计算`robustnest weight`:

$$\delta_{i}=B\left(e_{i} / 6 s\right)$$

其中，s为残差绝对值序列$|e_i|$d的中位值，B函数为bisquare函数:
$$B(u)=\left\{\begin{array}{cc}
\left(1-u^{2}\right)^{2} & \text { for } \quad 0 \leq u<1 \\
0 & \text { for } \quad u \geq 1
\end{array}\right.$$

然后，用robustness weight乘以kernel weight作为Wx0的新weight。如此，便剔除了残差较大的异常点对于回归的影响。

参考
-----------
作者：随风
链接：https://zhuanlan.zhihu.com/p/60648709
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。