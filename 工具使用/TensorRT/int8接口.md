## TensorRT-int8

基于`tensorRT`的python库和`Pycuda`的支持。

### tensorRT的int8介绍

#### representation

它使用线性量化方式$Tensor Values = FP32 scale * int8 array + FP32 bias$。

考虑两个矩阵:`A = scale_A * QA, B = scale_B * QB` 如果要计算两个矩阵的乘法

$$A * B = scale_A * scale_B * QA * QB + scale_A * QA * bias_B +  scale_B * QB * bias_A + bias_A * bias_B$$
后面的余项就会很浪费计算， 因此我们直接规定`A = scale_A * QA`，这样的话减少了计算量。

最简单的方案（非饱和截取）:
![](https://i.loli.net/2019/11/08/J6DTNyrtQGOL7Zi.png)

其方案是包含将所有值都包含进来然后映射到`[-127, 127]` （这里去掉-128只是为了方便计算， 因为加上-128会变的不对称）。如图上面有很大一部分是被浪费掉了，而且这种一遍只适合权重均匀分布，如果不均匀的话会产生很多误差。之后引入下图着这种有阈值的量化方法（饱和截取）。

![](https://i.loli.net/2019/11/08/iWvudMj3lQB2wf6.png)

问题来了，如何去找最优的阈值->（优化问题）。

另外的为问题，如果极端不均匀此种方法还是无法解决问题。(由于使用KL散度的问题，在优化过程中，它会主动的丢弃偏差大的值)

#### 如何去寻找阈值

想法：让截取之后量化之后得到的分布与越接近代表这个量化越OK，如何描述分布的接近程度呢？->`KL散度` $D_{K L}(p \| q)=\sum_{i=1}^{N} p\left(x_{i}\right) \cdot\left(\log p\left(x_{i}\right)-\log q\left(x_{i}\right)\right)$

处理细节：FP32是连续的如何去计算它的频率，切分成小区间NVIDIA实现的时候，切分了2048个区间，maxnet切分使用8000个区间，

#### 大致步骤

首先引入`calibration dataset`用于量化过程。
1. 收集激活值的直方图
2. 基于不同的阈值产生不同的量化分布
3. 计算KL，选择最少的那个

`IInt8EntropyCalibrator`类可以由接口需要由客户端实现，以提供校准数据集和一些用于缓存校准结果的样板代码。

校准过程

![](https://i.loli.net/2019/11/08/StPx2L4cOEgHs8k.png)

>1. 首先不断地截断参考样本P，长度从128开始到2048，为什么从128开始呢？因为截断的长度为128的话，那么我们直接一一对应就好了，完全不用衰减因子了；
>2. 将截断区外的值全部求和；
>3. 截断区外的值加到截断样本P的最后一个值之上；（截断区之外的值为什么要加到截断区内最后一个值呢？我个人理解就是有两个原因，其一是求P的概率分布时，需要总的P总值，其二将截断区之外的加到截断P的最后，这样是尽可能地将截断后的信息给加进来。）
>4. 求得样本P的概率分布；
>5. 创建样本Q，其元素的值为截断样本P的int8量化值；
>6. 将Q样本长度拓展到 i ，使得和原样本P具有相同长度；
>7. 求得Q的概率分布；
>8. 然后就求P、Q的KL散度值就好啦~
>上面就是一个循环，不断地构造P和Q，并计算相对熵，然后找到最小（截断长度为m）的相对熵，此时表示Q能极好地拟合P分布了。而阀值就等于（m + 0.5）*一个bin的长度；

转至zhihu:https://zhuanlan.zhihu.com/p/58182172#comments

### 处理的详细过程

bins越多，越真实，但带来的计算难度就越大

#### 激活值计算问题

1. 激活值的直方分布到底取正半区还是负半区? 大部分网络都用的ReLU，它只有正半区间。（Nvidia方案的局限）

如何处理->考虑负半轴

但是在vgg19也使用正半轴去做量化，有问题吗。

> 假如是非溢出实现，这没什么问题，反正负值都会丢掉的嘛，但是如果是溢出实现的话，则可能会有精度大幅损失的风险。
> 非溢出实现：无论正负值，在最大值处会报错饱和，那么在经过各类激活之后输出还是对的。
> 溢出实现：假如在计算负值时出现了溢出，那么其值就会由负数变为正数，极性都反了，那么很有可能对结果的值分布产生很大的影响。

工程问题：
input Activation 全是正的，但是weights并未全正。因此weights*ScaleW * blob*ScaleBlob其输出out FeatureMap的值是有正有负的，此时ScaleBlob的参数一旦选择不好的话，数据跟就上面的例子一样，溢出了呀！！！因此在激活之前就出现了溢出的话，后面再经ReLU，这个信息丢失还是有风险的呀！


> https://zhuanlan.zhihu.com/p/58182172#comments
> https://zhuanlan.zhihu.com/p/58208691