
### Post Training Dynamic Quantization

这是最简单的量化形式，其中权重是提前量化的，而激活是在推理过程中动态量化的。这用于以下情况：模型执行时间主要是通过从内存中加载权重而不是计算矩阵乘法来决定的。

### Post Training Static Quantization

这是最常用的量化形式，其中权重是提前量化的，并且基于在校准过程中观察模型的行为来预先计算激活张量的比例因子和偏差。(Nvidia)。用于内存和计算都紧缺的情况。
1. prepare the model:
   1. 通过添加QuantStub和DeQuantStub模块，指定在何处显式量化激活和量化数量。
   2. 模型可重用。
   3. 将所有需要重新量化的操作转换为模块。
2. 融合算子
3. 配置量化，例如选择对称或非对称量化以及MinMax或L2Norm校准技术。
4. 使用torch.quantization.prepare插入模块
5. 使用calibration set去校准
6. 最后，使用`torch.quantization.convert()`方法转换模型本身。这可以做几件事：量化权重，计算并存储每个激活张量要使用的比例和偏差值，并替换关键算子的量化实现。

### Quantization Aware Training

需要使用`torch.quantization.FakeQuantize`
1. （1）（2）头相同
2. 指定伪量化方法'97的配置，例如选择对称或非对称量化以及MinMax或移动平均或L2Norm校准技术。
3. 使用torch.quantization.prepare_qat()去插入将在训练过程中模拟量化的模块。
4. 训练
5. 与（6）相同

整个过程提供pre-channel的conv2d和linear()
