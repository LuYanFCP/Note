# Quantized Inference: Performance and Accuracy

## Post-training quantization

就是讲训练好的模型的参数直接通过某些方式进行量化，不再重新训练。
**pre-training with asymmetric ranges(8-bit) 很接近全精度训练**

在量化激活值的时候需要->由于激活需要量化，因此需要校准数据并需要计算激活的动态范围 (为什么？)

### 实验结果

1. 只量化权重的

![只量化权重](https://i.loli.net/2019/10/25/CKaE3FkhzgvTx4H.png)

可以看出，非对称pre-channel的量化效果是最好的

2. 权值和激活值都量化

使用pre-layer quantization 量化 activations （为什么？）
使用层和通道的粒度上的对称和非对称量化

![权值和激活值量化](https://i.loli.net/2019/10/25/Y2G3NLbpuHraVs9.png)

pre-channel的权值量化，使用asymmetric的方法会提高一些效果

### 总结

1. Pre-channel效果好于Pre-layer，它可以提供在8-bit级别量化后acc基本接近全精度的训练效果。asymmetric的量化效果略好与symmetric量化。
2. 激活值可以无论的量化到8-bit。（注激活值的范围一般较小：BN保证了这一点， 一些网络使用ReLu6（mobilenet）优化了这一点）。
3. 参数多的网络比轻量级网络量化后鲁棒性高。
4. 大部分量化误差是权重造成的。 （有争议，为什么？）

权值的pre-layer量化损失大的主要原因是因为BN，它会在单层卷积核之间动态范围的极端变化。pre-channel通过量化一个kernel的粒度回避了这个问题。（为什么？）。
使用权值正则化会提升模型的效果。

## Quantization Aware Training

量化感知训练在量化后还继续训练（微调）。可以使用从头开始训练和使用pre-train模型进行训练。
量化感知训练在4位精度下，量化感知训练比训练后的量化方案有显著的改进。

向前传播的时候使用伪量化操作，向后传播的时候使用STE方法。对于SGD梯度如下：
![](https://i.loli.net/2019/10/26/KV69RFxpckmnCYE.png)

## 量化操作符的转换

核心是算符融合。例如add操作一般是跟一个relu，因此可以讲两者融合。为了匹配这一点，不应该在加法和ReLU操作之间放置伪量化操作。

## BN

BN的基本定义：
在训练时候：

![](https://i.loli.net/2019/10/26/bqgxJhMKzked4LP.png)

在推理时：

![](https://i.loli.net/2019/10/26/mbkX8DleJ3qx7nv.png)

其中$\mu_{B}$和$\sigma_{B}s$是每一批的平均数和标准差，$\mu$和$\sigma$是长期平均值、标准差，一般使用滑动平均确定。

为了推理方便把BN化为一个Linear操作:

![](https://i.loli.net/2019/10/26/wja64eQoHXTVxZG.png)

1. 量化之前使用一个校正因子来衡量长期统计数据的权重。这确保量化时权重不会那么抖动。（重点，为了防止对权重量化的影响）

![](https://i.loli.net/2019/10/26/fwOWSPFN6r8zuDI.png)

2. 在训练的初始阶段，我们撤消了权值的缩放，以便输出与常规批处理规范化相同。我们也相应地修改了偏差项。（因为训练的时候用的还应该是$\mu_{B}$，因此在conv之后立即转换回去）

![](https://i.loli.net/2019/10/26/tb3Hi5jw7nfaIS4.png)

3. 在充分训练之后，然后转换为使用长期滑动平均的BN（注意，长期平均值被冻结以避免训练中的不稳定性。这对应于推理时使用的规范化参数，并提供稳定的性能）。

![](https://i.loli.net/2019/10/26/76YzGkFZKxJUD9b.png)

### 实践

实验条件：
1. 使用model zoo中模型进行微调
2. 使用SGD进行微调，步长为1e-5

发现
1. 训练消除了symmetric和asymmetric的差距
2. 训练允许更简单的量化方案提供接近浮点精度。甚至每层量化显示接近浮点精度

![](https://i.loli.net/2019/10/26/B2qwIuCWO5HsDgc.png)

### 低精度网络

我们注意到，在精度为8位的情况下，训练后的量化方案提供了接近浮点数的精度。为了更好地理解量化感知训练的好处，我们进行了实验来评估4位量化的权重和激活的性能。

1. Pre-channel在更低比特上，要更优于pre-channel的量化方法。
![](https://i.loli.net/2019/10/26/YODwq9rCxdzbATy.png)
2. 在使用fine tuning会让低比特条件下精度得到显著提高。
3. 激活值量化造成的损失比权重量化更严重。如下图：
![](https://i.loli.net/2019/10/26/Y93RktUaJwP14KW.png)