## Introduce

两种思路：
1. 减少模型大小和复杂度：quantization、pruning、compression
2. faster inference： GEMMLOWP（google）、MKL-DNN（Intel）、ARM CMSIS、Qualcomm SNPE、TensorRT（nvidia）。

降低任何模型复杂性的一种更简单的方法是降低对权重和激活的精度要求。这种方法有很多优点：
1. 可以广泛使用它，不需要创建一种新的设计模型结构。在许多情况下，可以从现有的浮点模型开始，快速地对其进行量化，以获得几乎没有精度损失的定点量化模型，而不需要对模型进行重新训练。（8-bit）。而且多个硬件屏幕和库都支持量化权重和激活值的快速推理，不要新的硬件平台的开发。
2. 更小的内存占用。以为量化会减少模型的大小，因此可以减少内存的占用。
3. 对cache更加友好。体积小的话能更多被掉进cache内，从而加速。
4. 更快的运算。
5. 更低的能耗。体积小，数据移动量小。减少数据移动量会对功耗产生显著影响。

------------
一些自己的总结：
一般来说从量化后是否在训练（训练）来说量化分为两种
1. post-quantization 使用pre-train模型进行量化直接进行量化。
2. quantization-aware training 量化后需要训练，他通过伪量化操作进行训练。它也分为两种：一种是从0开始训练，另一种是使用预训练模型微调。

从粒度方面有可以分为：
1. pre-layer
2. pre-channel

按量化形式上又可以分为
1. uniform quantization [大部分论文都是]
2. NonUniform quantization [hansong的压缩论文 2016]
--------------------

## Qunatizer Design

### Uniform Affine Quantizer

将$(x_{min}, x_{max})$量化到$(0, N_{levels}-1)$ 对于8bit量化来说$N_{level}=256$。量化过程主要需要两个参数$scala:\delta$和$zero-point: \delta$，scala为量化的步长，zero-point为float零点对应的量化后整数的位置，**一般来说量化零点应该保证无误差**。这非常重要因为在CNN等网络中有大量的padding操作，如果零点有误差会积累很多误差。

对与单侧分布，比如说$(2.1, 3.1)$这些最大值最小值都大于0，由于上述原因在，量化的时候必须将此类分布变成$(0, 3.1)$然后再进行量化。（请注意，在极端单边分布的情况下，这可能会导致精度损失）。

![量化的基本公式](https://i.loli.net/2019/10/23/pj4d1eMOy6YNKU2.png)

在conv操作下的8bit公式：

![量化conv](https://i.loli.net/2019/10/23/I6RktLufjaxJViN.png)

### Uniform symmetric quantizer

严格的令zero-point为0，因此整个量化的公式就可以写成：

![](https://i.loli.net/2019/10/23/7Dt6fLbRCg1ZhFp.png)

进一步，如果想使用SIMD实现，需要进一步的限制范围为

![](https://i.loli.net/2019/10/23/yV8FDwROSXBdr4h.png)

因此de-quantization表达式为:
$$x_{out} = x_{Q} \delta$$

### Stochastic quantizer

随机量化就是在量化的原值上加一个噪声，即：
![](https://i.loli.net/2019/10/23/176jhvHKEynexFR.png)

---
这部分还需要再看
----

### Modeling simulated quantization in the backward pass

主要针对着quantization-aware training。它使用伪量化技术。

![](https://i.loli.net/2019/10/23//ph9QU5PI1wyaqKo.png)

伪量化示意图:
![](https://i.loli.net/2019/10/24/nOJZN7wemQqAVsC.png)
由图可以看出，这个量化函数导数一直是0，因此在伪量化反向传播时就会出现问题。无法进行梯度下降操作。因此在反向传播时会对模型做一个`straight through estimator`方法，也就是将$x_{out}$变成$x_{out} = clamp(x_{min}, x_{max}, x)$的方式，如图：

![](https://i.loli.net/2019/10/24/gLD5uHeCXNrAjxG.png)

这样的话，梯度就变成

![](https://i.loli.net/2019/10/24/nZkYFPlD8tELiOV.png)

> ICCV和CVPR的两篇论文就是整理这个方向的进展，用新的方法处理反向传播的问题

### Determing Quantizer parameters

量化的时候需要确定很多参数，比如说$\Delta$和。

TensorRT最小化 量化之前和量化之后分布的KL散度对 `step size`进行确定。-> post training quantization-> 选择最优秀的参数来提高效果。
权重使用最大值最小值来确定相关参数。
激活值使用最小值和最大值的移动平均值来确定量化器参数。

### Granularity of quantization

+ pre-layer: 对每一个张量进行量化
+ pre-channel: 对于每一个conv-kernel有不同的尺度和偏移量。