### 具体方法

```python
# 线性量化
def linear_quantize(input, sf, bits):
    assert bits >= 1, bits
    # 一位
    if bits == 1:
        return torch.sign(input) - 1
    
    delta = math.pow(2.0, -sf)# 小数位 位宽 量化精度
    bound = math.pow(2.0, bits-1)
    min_val = - bound    # 上限制值
    max_val = bound - 1  # 下限值
    rounded = torch.floor(input / delta + 0.5)# 扩大后取整

    clipped_value = torch.clamp(rounded, min_val, max_val) * delta# 再缩回
    return clipped_value
# 对数线性量化
def log_linear_quantize(input, sf, bits):
    assert bits >= 1, bits
    if bits == 1:
        return torch.sign(input), 0.0, 0.0

    s = torch.sign(input)# 正负号
    input0 = torch.log(torch.abs(input) + 1e-20)# 求对数 获取 比特位
    v = linear_quantize(input0, sf, bits)# 对比特位进行线性量化
    v = torch.exp(v) * s# 再指数 回 原数
    return v
# 双曲正切量化
def tanh_quantize(input, bits):
    assert bits >= 1, bits
    if bits == 1:
        return torch.sign(input)

    input = torch.tanh(input) # 双曲正切 映射 [-1, 1]
    input_rescale = (input + 1.0) / 2 #  再 映射到 [0, 1]
    n = math.pow(2.0, bits) - 1       # 固定比特位 放大系数
    v = torch.floor(input_rescale * n + 0.5) / n # 放大后取整
    v = 2 * v - 1 # [-1, 1]                      # 再放回原来的范围

    v = 0.5 * torch.log((1 + v) / (1 - v))       # 反双曲正切 回原数 arctanh
    return v
```


## INT量化

*On the efficient representation and execution of deep acoustic models*

量化方法的目的就是使用 8 位或 16 位的整型数来替代浮点数，这种方法试图利用定点点积来替代浮点点积，这很大程度上降低了神经网络在无硬浮点设备上的运算开销。同时，该方法在一些支持单指令流多数据流 SIMD 的硬件设备上优势就更加明显了，比如128-bit 寄存器 SSE 可以单个指令同时运算 4 个 32 位单精度浮点，8 个 16 位整型，16 个 8 位整型。显然 8 位整型数在 SIMD 的加持下，相比于单精度浮点运算速率要更快一些。另外，该方法还可以减少模型的内存和存储占用空间。

量化过程

$$vq = \frac{V_x - V_{min}}{V_{max} - V_{min}} * 2^{bits - 1}$$

https://zhuanlan.zhihu.com/p/38328685

## UINT量化

《Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference》

使用量化感知训练的化可以无视掉是否对称的问题。
这篇论文的另一个贡献在于：原先的INT8量化是针对已经训练好的模型。而现在还可以在训练的时候就进行量化——前向计算进行量化，而反向的误差修正不做量化。

## 量化

### 二值网络

![](https://camo.githubusercontent.com/44a8c438d328b01a72e2e307ea416f8f61fcebe7/687474703a2f2f66696c652e656c656366616e732e636f6d2f776562312f4d30302f35352f37392f7049594241467373565f53416455364241414363764477473570553637372e706e67)

上图是在定点表示里面最基本的方法：BNN和BWN。
在网络进行计算的过程中，可以使用定点的数据进行计算，
由于是定点计算，实际上是不可导的，
于是提出使用straight-through方法将输出的估计值直接传给输入层做梯度估计。
在网络训练过程中会保存两份权值，用定点的权值做网络前向后向的计算，
整个梯度累积到浮点的权值上，整个网络就可以很好地训练，
后面几乎所有的量化方法都会沿用这种训练的策略。
前面包括BNN这种网络在小数据集上可以达到跟全精度网络持平的精度，
但是在ImageNet这种大数据集上还是表现比较差。

二值化神经网络，是指在浮点型神经网络的基础上，
将其权重矩阵中权重值(线段上) 和 各个 激活函数值(圆圈内) 同时进行二值化得到的神经网络。
1. 一个是存储量减少，一个权重使用 1bit 就可以，而原来的浮点数需要32bits。
2. 运算量减少， 原先浮点数的乘法运算，可以变成 二进制位的异或运算。

BNN的 激活函数值 和 权重参数都被二值化了, 前向传播是使用二值，反向传播时使用全精度梯度。

[BNN的pytorch实现](https://github.com/Ewenwan/pytorch_workplace/tree/master/binary)
[随机二值化](https://github.com/Ewenwan/BinaryNet-1)
[基于CUDA的二值化GPU运算代码，推理](https://github.com/MatthieuCourbariaux/BinaryNet/blob/master/Run-time/binary_kernels.cu)
[基于CPU的运算实现，推理](https://github.com/codekansas/tinier-nn/tree/master/eval)

#### 二值化方法
1. 阈值二值化，确定性(sign()函数) x>0 为1，其他为-1
2. 基于概率二值化 x = 1 p = sigmod(x) -1 1-p

#### 如何训练

提出的解决方案是：权值和梯度在训练过程中保持全精度（full precison），也即，训练过程中，权重依然为浮点数，训练完成后，再将权值二值化，以用于 inference。

在训练过程中，权值为 32 位的浮点数，取值值限制在 [-1, 1] 之间，以保持网络的稳定性,为此，训练过程中，每次权值更新后，需要对权值 W 的大小进行检查，W=max(min(1,W),−1)。

前向运算时，我们首先得到二值化的权值：Wkb=sign(Wk),k=1,⋯,n，然后，用 Wkb 代替 Wk：xk=σ(BN(Wkb * xk−1)=sign(BN(Wkb * xk−1)) 其中，BN(⋅) 为 Batch Normalization 操作。

先前传播
![](https://camo.githubusercontent.com/9b9cf10cbbd4bee395a619b11a7ac718f68a15ac/68747470733a2f2f696d672d626c6f672e6373646e2e6e65742f32303137303231343031303133393630373f77617465726d61726b2f322f746578742f6148523063446f764c324a736232637559334e6b626935755a585176644746755a33646c615449774d54513d2f666f6e742f3561364c354c32542f666f6e7473697a652f3430302f66696c6c2f49304a42516b46434d413d3d2f646973736f6c76652f37302f677261766974792f536f75746845617374)

反向传播

![]()


### ADMM

目标函数：
$$\min _{W} f(W) \quad \text { s.t. } W \in \mathcal{C}=\{-1,0,+1\}^{d}$$